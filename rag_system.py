"""
å°è©±å¼ RAG ç³»çµ±ï¼šä½¿ç”¨ ComplementaryRetriever + TAIDE LLM (4-bit é‡åŒ–ç‰ˆæœ¬)
æ”¯æ´å¤šè¼ªå°è©±èˆ‡ä¸Šä¸‹æ–‡è¨˜æ†¶
é¡¯ç¤ºæª¢ç´¢åˆ°çš„åƒè€ƒæ–‡ç« 
"""

import os
from typing import List, Dict, Optional, Tuple
from complementary_retriever import ComplementaryRetriever
from transformers import AutoTokenizer, AutoModelForCausalLM
from complementary_retriever import ComplementaryRetriever, BM25Index
#from transformers import BitsAndBytesConfig
import torch
import sys
import os
from dotenv import load_dotenv
load_dotenv()
sys.modules['__main__'].BM25Index = BM25Index

class ConversationalRAG:
    """å°è©±å¼ RAG ç³»çµ±"""

    def __init__(
        self,
        faiss_index_path: str,
        metadata_path: str,
        bm25_index_path: str,
        hf_token: str,
        model_name: str = "taide/TAIDE-LX-7B-Chat"
        # use_4bit: bool = True  # â† åˆªé™¤é€™å€‹åƒæ•¸
    ):
        # åˆå§‹åŒ–æª¢ç´¢å™¨
        print("=" * 60)
        print("åˆå§‹åŒ–æª¢ç´¢å™¨...")
        print("=" * 60)
        self.retriever = ComplementaryRetriever(
            faiss_index_path=faiss_index_path,
            metadata_path=metadata_path,
            bm25_index_path=bm25_index_path
        )
        
        # åˆå§‹åŒ– TAIDE LLM
        print("\n" + "=" * 60)
        print("è¼‰å…¥ TAIDE æ¨¡å‹ï¼ˆç„¡é‡åŒ–ï¼‰...")
        print("=" * 60)
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è£ç½®: {self.device}")
        
        # è¼‰å…¥ Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=hf_token
        )
        
        # è¼‰å…¥æ¨¡å‹ï¼ˆç„¡é‡åŒ–ï¼‰
        print("æ¨¡å‹é…ç½®:")
        print("  - ç²¾åº¦: float16" if self.device == "cuda" else "  - ç²¾åº¦: float32")
        print("  - è¨­å‚™åˆ†é…: auto")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            token=hf_token,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        print("\nâœ“ TAIDE æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # é¡¯ç¤º GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
        if self.device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"âœ“ GPU è¨˜æ†¶é«”ä½¿ç”¨: {allocated:.2f} GB (å·²é…ç½®) / {reserved:.2f} GB (å·²ä¿ç•™)")
        print()
        
        # å°è©±æ­·å²
        self.conversation_history = []
        self.max_history_turns = 5

    def _detect_query_type(self, query: str) -> str:
        """åˆ¤æ–·æŸ¥è©¢é¡å‹"""
        temporal_keywords = ['ç¬¬ä¸€å€‹', 'èª°å…ˆ', 'æœ€æ—©', 'èµ·æº', 'é–‹å§‹', 'ä½•æ™‚']
        if any(kw in query for kw in temporal_keywords):
            return 'temporal'
        return 'general'

    def _format_context(self, results: List[Dict], query_type: str) -> str:
        """æ ¼å¼åŒ–æª¢ç´¢çµæœç‚º Context"""
        if not results:
            return "ï¼ˆæœªæ‰¾åˆ°ç›¸é—œè³‡æ–™ï¼‰"

        parts = []

        if query_type == 'temporal':
            results_sorted = sorted(results, key=lambda x: x.get('push_time', ''))
            parts.append("ä»¥ä¸‹æ˜¯æŒ‰æ™‚é–“æ’åºçš„ç›¸é—œè¨è«–ï¼š\n")

            for i, r in enumerate(results_sorted[:10], 1):
                parts.append(
                    f"{i}. [{r['push_time']}] {r['user_id']}\n"
                    f"   {r['combined_text'][:150]}\n"
                )
        else:
            parts.append("ä»¥ä¸‹æ˜¯ç›¸é—œçš„è¨è«–å…§å®¹ï¼š\n")

            for i, r in enumerate(results[:15], 1):
                source_label = {
                    'bm25_high': 'ç²¾ç¢º',
                    'bm25_med': 'éƒ¨åˆ†',
                    'faiss_high': 'ç›¸é—œ',
                    'faiss_low': 'åƒè€ƒ'
                }.get(r.get('source', ''), 'åƒè€ƒ')

                parts.append(f"{i}. [{source_label}] {r['combined_text'][:120]}\n")

        return "\n".join(parts)

    def _format_retrieved_documents(self, results: List[Dict], query_type: str) -> str:
        """æ ¼å¼åŒ–æª¢ç´¢æ–‡æª”ä¾›ä½¿ç”¨è€…æŸ¥çœ‹ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰"""
        if not results:
            return "æœªæª¢ç´¢åˆ°ç›¸é—œæ–‡ç« "

        parts = ["\n" + "=" * 60]
        parts.append("ğŸ“š æª¢ç´¢åˆ°çš„åƒè€ƒæ–‡ç« ï¼ˆLLM é–±è®€çš„å…§å®¹ï¼‰")
        parts.append("=" * 60 + "\n")

        if query_type == 'temporal':
            results_sorted = sorted(results, key=lambda x: x.get('push_time', ''))
            results_to_show = results_sorted[:10]
        else:
            results_to_show = results[:15]

        for i, r in enumerate(results_to_show, 1):
            # ä¾†æºæ¨™ç±¤
            source_label = {
                'bm25_high': 'ğŸ¯ ç²¾ç¢ºåŒ¹é…',
                'bm25_med': 'ğŸ“Œ éƒ¨åˆ†åŒ¹é…',
                'faiss_high': 'ğŸ”— é«˜åº¦ç›¸é—œ',
                'faiss_low': 'ğŸ“ åƒè€ƒç›¸é—œ'
            }.get(r.get('source', ''), 'ğŸ“ åƒè€ƒ')

            # æ–‡ç« è³‡è¨Š
            parts.append(f"ã€æ–‡ç«  {i}ã€‘ {source_label}")
            parts.append(f"ä½œè€…: {r.get('user_id', 'Unknown')}")
            parts.append(f"æ™‚é–“: {r.get('push_time', 'Unknown')}")

            # ç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'score' in r:
                parts.append(f"ç›¸ä¼¼åº¦: {r['score']:.4f}")

            parts.append(f"\nå…§å®¹:")
            parts.append(f"{r['combined_text']}")
            parts.append("\n" + "-" * 60 + "\n")

        return "\n".join(parts)

    def _build_prompt(
            self,
            query: str,
            context: str,
            query_type: str,
            use_history: bool = True  # â† é€™å€‹åƒæ•¸å¯ä»¥ä¿ç•™ä½†ä¸æœƒä½¿ç”¨
    ) -> str:
        """å»ºæ§‹å°è©±å¼ Prompt"""

        system_prompt = """ä½ æ˜¯ PTT Baseball æ¿çš„å°ˆå®¶åŠ©æ‰‹ï¼Œå°ˆé–€å›ç­”æ£’çƒç›¸é—œå•é¡Œã€‚
    ä½ çš„å›ç­”é¢¨æ ¼ï¼š
    - è¦ªåˆ‡è‡ªç„¶ï¼Œåƒæœ‹å‹èŠå¤©
    - æ ¹æ“šåƒè€ƒè³‡æ–™å›ç­”ï¼Œä¸ç·¨é€ è³‡è¨Š
    - å¦‚æœä¸ç¢ºå®šï¼Œèª å¯¦èªªæ˜
    - å¯ä»¥é©åº¦åŠ å…¥è‡ªå·±çš„è¦‹è§£
    - å›ç­”ç°¡æ½”æ¸…æ¥šï¼Œä¸è¦éæ–¼å†—é•·"""

        # â† åˆªé™¤å°è©±æ­·å²éƒ¨åˆ†
        # history_section = ""
        # if use_history and self.conversation_history:
        #     history_section = f"\n{self._format_conversation_history()}\n"

        context_section = f"""
    ã€åƒè€ƒè³‡æ–™ã€‘
    {context}
    """

        if query_type == 'temporal':
            special_instruction = "\nè«‹æ˜ç¢ºæŒ‡å‡ºç¬¬ä¸€å€‹æåˆ°çš„äººå’Œæ™‚é–“ã€‚"
        else:
            special_instruction = ""

        # â† ç§»é™¤ history_section
        prompt = f"""[INST] <<SYS>>
    {system_prompt}
    <</SYS>>

    {context_section}

    ã€ä½¿ç”¨è€…å•é¡Œã€‘
    {query}{special_instruction}
    [/INST]"""

        return prompt

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """ä½¿ç”¨ TAIDE ç”Ÿæˆå›ç­”"""

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "[/INST]" in response:
            response = response.split("[/INST]")[-1].strip()

        return response

    def chat(
        self,
        question: str,
        top_k: int = 20,
        bm25_high_threshold: float = 16.0,
        bm25_med_threshold: float = 12.0,
        faiss_high_threshold: float = 0.90,
        faiss_low_threshold: float = 0.80,
        use_retrieval: bool = True,
        use_history: bool = True,
        show_retrieved_docs: bool = True  # æ–°å¢ï¼šæ˜¯å¦é¡¯ç¤ºæª¢ç´¢æ–‡æª”
    ) -> Tuple[str, Optional[str], Optional[List[Dict]]]:
        """
        å°è©±å¼æŸ¥è©¢ï¼ˆä¸»è¦ä»‹é¢ï¼‰

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            use_retrieval: æ˜¯å¦ä½¿ç”¨ RAG æª¢ç´¢
            use_history: æ˜¯å¦ä½¿ç”¨å°è©±æ­·å²
            show_retrieved_docs: æ˜¯å¦è¿”å›æª¢ç´¢åˆ°çš„æ–‡æª”

        Returns:
            Tuple[answer, retrieved_docs_text, results]:
                - answer: LLM ç”Ÿæˆçš„å›ç­”
                - retrieved_docs_text: æ ¼å¼åŒ–çš„æª¢ç´¢æ–‡æª”æ–‡å­—ï¼ˆä¾›é¡¯ç¤ºï¼‰
                - results: åŸå§‹æª¢ç´¢çµæœåˆ—è¡¨
        """
        # åˆ¤æ–·æŸ¥è©¢é¡å‹
        query_type = self._detect_query_type(question)

        # æª¢ç´¢ç›¸é—œæ–‡æª”
        context = ""
        results = None
        retrieved_docs_text = None

        if use_retrieval:
            results = self.retriever.search(
                query=question,
                top_k=top_k,
                bm25_high_threshold=bm25_high_threshold,
                bm25_med_threshold=bm25_med_threshold,
                faiss_high_threshold=faiss_high_threshold,
                faiss_low_threshold=faiss_low_threshold
            )

            if results:
                context = self._format_context(results, query_type)

                # æ ¼å¼åŒ–æª¢ç´¢æ–‡æª”ä¾›ä½¿ç”¨è€…æŸ¥çœ‹
                if show_retrieved_docs:
                    retrieved_docs_text = self._format_retrieved_documents(results, query_type)

        # å»ºæ§‹ Prompt
        prompt = self._build_prompt(question, context, query_type, use_history)

        # ç”Ÿæˆå›ç­”
        answer = self.generate(prompt)

        return answer, retrieved_docs_text, results

    def clear_history(self):
        """æ¸…ç©ºå°è©±æ­·å²"""
        print("å°è©±è¨˜æ†¶åŠŸèƒ½å·²åœç”¨")


def interactive_chat():
    """äº’å‹•å¼å°è©±"""

    # åˆå§‹åŒ–ç³»çµ±
    rag = ConversationalRAG(
        faiss_index_path="faiss_index_part1/ptt_baseball_part1.index",
        metadata_path="faiss_index_part1/ptt_baseball_metadata_part1.pkl",
        bm25_index_path="faiss_index_part1/bm25_index.pkl",
        hf_token=os.environ.get("HUGGINGFACE_TOKEN")
    )

    print("\n" + "=" * 60)
    print("PTT Baseball å°è©±åŠ©æ‰‹ï¼ˆè¼¸å…¥ 'q' é€€å‡ºï¼‰")
    print("=" * 60)
    print("\næŒ‡ä»¤ï¼š")
    print("  q      - é€€å‡º")
    print("  clear  - æ¸…ç©ºå°è©±æ­·å²")
    print("  help   - é¡¯ç¤ºå¹«åŠ©")
    print("  docs   - åˆ‡æ›æ˜¯å¦é¡¯ç¤ºæª¢ç´¢æ–‡æª”\n")

    show_docs = True  # é è¨­é¡¯ç¤ºæª¢ç´¢æ–‡æª”

    while True:
        question = input("\nä½ : ").strip()

        if question.lower() == 'q':
            print("\nå†è¦‹ï¼")
            break

        if question.lower() == 'clear':
            rag.clear_history()
            continue

        if question.lower() == 'docs':
            show_docs = not show_docs
            status = "é–‹å•Ÿ" if show_docs else "é—œé–‰"
            print(f"\nå·²{status}æª¢ç´¢æ–‡æª”é¡¯ç¤º")
            continue

        if question.lower() == 'help':
            print("\nä½ å¯ä»¥å•æˆ‘ï¼š")
            print("  - çƒå“¡ç›¸é—œå•é¡Œï¼ˆç‹å»ºæ°‘è¡¨ç¾å¦‚ä½•ï¼Ÿï¼‰")
            print("  - æ™‚åºå•é¡Œï¼ˆWang Soto èª°å…ˆæå‡ºï¼Ÿï¼‰")
            print("  - ä¸€èˆ¬è¨è«–ï¼ˆå¤§è°·ç¿”å¹³è©•åƒ¹å¦‚ä½•ï¼Ÿï¼‰")
            print("  - ä¹Ÿå¯ä»¥ç´”èŠå¤©ï¼ˆä¸ä¸€å®šè¦æ£’çƒç›¸é—œï¼‰")
            continue

        if not question:
            continue

        try:
            # ç”Ÿæˆå›ç­”ï¼ˆå–å¾—ä¸‰å€‹è¿”å›å€¼ï¼‰
            answer, retrieved_docs, results = rag.chat(
                question,
                show_retrieved_docs=show_docs
            )

            # å…ˆé¡¯ç¤º LLM å›ç­”
            print(f"\nğŸ¤– åŠ©æ‰‹å›ç­”:")
            print("=" * 60)
            print(answer)
            print("=" * 60)

            # é¡¯ç¤ºæª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if show_docs and retrieved_docs:
                print(retrieved_docs)

            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
            if results:
                print(f"\nğŸ“Š æª¢ç´¢çµ±è¨ˆ: å…± {len(results)} ç¯‡æ–‡ç« ")

        except Exception as e:
            print(f"\néŒ¯èª¤ï¼š{e}")
            import traceback
            traceback.print_exc()


def demo_conversation():
    """ç¤ºç¯„å°è©±"""

    rag = ConversationalRAG(
        faiss_index_path="faiss_index_part1/ptt_baseball_part1.index",
        metadata_path="faiss_index_part1/ptt_baseball_metadata_part1.pkl",
        bm25_index_path="faiss_index_part1/bm25_index.pkl",
        hf_token="token"
    )

    conversation = [
        "ç‹å»ºæ°‘è¡¨ç¾å¦‚ä½•ï¼Ÿ",
        "ä»–è·Ÿéƒ­æ³“å¿—æ¯”å‘¢ï¼Ÿ",
        "é‚£å¤§è°·ç¿”å¹³å‘¢ï¼Ÿ",
    ]

    print("\n" + "=" * 60)
    print("ç¤ºç¯„å°è©±")
    print("=" * 60)

    for question in conversation:
        print(f"\nä½¿ç”¨è€…: {question}")

        # å–å¾—å›ç­”å’Œæª¢ç´¢æ–‡æª”
        answer, retrieved_docs, results = rag.chat(question, show_retrieved_docs=True)

        print(f"\nğŸ¤– åŠ©æ‰‹: {answer}")
        # é¡¯ç¤ºæª¢ç´¢çš„æ–‡ç« 
        if retrieved_docs:
            print(retrieved_docs)

        input("\næŒ‰ Enter ç¹¼çºŒ...")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        demo_conversation()
    else:
        interactive_chat()