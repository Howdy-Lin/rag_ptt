{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254d7465-b1bf-4b81-8fe1-2760e2996acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從上次進度繼續：https://www.ptt.cc/bbs/Baseball/index3198.html\n",
      "開始爬取 PTT 棒球版，目標日期：2020-01-01 至今\n",
      "\n",
      "處理列表頁: https://www.ptt.cc/bbs/Baseball/index3198.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文章時間 (2019-12-31 20:08:38) 早於目標日期，停止爬取\n",
      "\n",
      "爬取結束\n",
      "  新增文章：0 篇\n",
      "  跳過已爬：0 篇\n",
      "  過濾文章：0 篇 (LIVE/公告)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "DB_NAME = 'ptt_baseball.db'\n",
    "\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"初始化資料庫和資料表\"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            article_id TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            author TEXT,\n",
    "            post_time TIMESTAMP,\n",
    "            board TEXT,\n",
    "            url TEXT UNIQUE\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS comments (\n",
    "            comment_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            article_id TEXT,\n",
    "            push_tag TEXT,\n",
    "            user_id TEXT,\n",
    "            content TEXT,\n",
    "            push_time TIMESTAMP,\n",
    "            FOREIGN KEY(article_id) REFERENCES articles(article_id)\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS metadata (\n",
    "            key TEXT PRIMARY KEY,\n",
    "            value TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"資料庫 '{DB_NAME}' 初始化完成。\")\n",
    "\n",
    "\n",
    "def save_progress(cursor, key, value):\n",
    "    \"\"\"儲存進度到 metadata 資料表\"\"\"\n",
    "    cursor.execute(\"INSERT OR REPLACE INTO metadata (key, value) VALUES (?, ?)\", (key, value))\n",
    "\n",
    "\n",
    "def get_progress(cursor):\n",
    "    \"\"\"從資料庫獲取上次的進度\"\"\"\n",
    "    cursor.execute(\"SELECT value FROM metadata WHERE key = 'last_page_url'\")\n",
    "    last_page_result = cursor.fetchone()\n",
    "    last_page_url = last_page_result[0] if last_page_result else None\n",
    "\n",
    "    cursor.execute(\"SELECT url FROM articles ORDER BY post_time DESC LIMIT 1\")\n",
    "    latest_article_result = cursor.fetchone()\n",
    "    latest_article_url = latest_article_result[0] if latest_article_result else None\n",
    "\n",
    "    return last_page_url, latest_article_url\n",
    "\n",
    "\n",
    "def is_article_exists(cursor, article_url):\n",
    "    \"\"\"快速檢查文章是否已存在資料庫\"\"\"\n",
    "    cursor.execute(\"SELECT 1 FROM articles WHERE url = ? LIMIT 1\", (article_url,))\n",
    "    return cursor.fetchone() is not None\n",
    "\n",
    "\n",
    "def parse_push_time(datetime_text, article_year):\n",
    "    \"\"\"解析留言時間\"\"\"\n",
    "    if not datetime_text:\n",
    "        return None\n",
    "    match = re.search(r'(\\d{2}/\\d{2})\\s+(\\d{2}:\\d{2})', datetime_text)\n",
    "    if not match:\n",
    "        return None\n",
    "    date_part, time_part = match.group(1), match.group(2)\n",
    "    try:\n",
    "        return datetime.strptime(f\"{article_year}/{date_part} {time_part}\", \"%Y/%m/%d %H:%M\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def should_skip_article(title):\n",
    "    \"\"\"判斷是否要跳過文章（LIVE 或公告）\"\"\"\n",
    "    title_lower = title.lower()\n",
    "\n",
    "    # 跳過 LIVE 文\n",
    "    if '[live]' in title_lower:\n",
    "        return True, \"LIVE\"\n",
    "\n",
    "    # 跳過公告（常見格式）\n",
    "    if any(keyword in title_lower for keyword in ['[公告]', '[協尋]', '[ 公告 ]']):\n",
    "        return True, \"公告\"\n",
    "\n",
    "    # 跳過置底文（通常是公告）\n",
    "    if title.startswith('□'):\n",
    "        return True, \"置底\"\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def get_article_data(session, article_url):\n",
    "    \"\"\"獲取單篇文章的完整資訊\"\"\"\n",
    "    try:\n",
    "        response = session.get(article_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        if '404 - Not Found.' in response.text:\n",
    "            return None, []\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None, []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    metaline_divs = soup.find_all('div', class_='article-metaline')\n",
    "    meta = {div.find('span', class_='article-meta-tag').text.strip():\n",
    "                div.find('span', class_='article-meta-value').text.strip()\n",
    "            for div in metaline_divs}\n",
    "\n",
    "    try:\n",
    "        post_time = datetime.strptime(meta.get('時間', ''), '%a %b %d %H:%M:%S %Y')\n",
    "        author = meta.get('作者', 'N/A').split(' ')[0]\n",
    "        title = meta.get('標題', 'N/A')\n",
    "        board = meta.get('看板', 'N/A')\n",
    "    except (ValueError, TypeError):\n",
    "        return None, []\n",
    "\n",
    "    match = re.search(r'/(M\\.\\d+\\.A\\.[A-Z0-9_]+)\\.html', article_url)\n",
    "    article_id = match.group(1) if match else \"未知ID\"\n",
    "\n",
    "    article_info = {\n",
    "        'article_id': article_id,\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'post_time': post_time,\n",
    "        'board': board,\n",
    "        'url': article_url\n",
    "    }\n",
    "\n",
    "    comments = []\n",
    "    for push in soup.find_all('div', class_='push'):\n",
    "        user_id_span = push.find('span', 'push-userid')\n",
    "        content_span = push.find('span', 'push-content')\n",
    "        if not all([user_id_span, content_span]):\n",
    "            continue\n",
    "\n",
    "        push_tag_span = push.find('span', class_='push-tag')\n",
    "        push_tag = push_tag_span.text.strip() if push_tag_span else '→'\n",
    "\n",
    "        push_time = None\n",
    "        ipdatetime_span = push.find('span', 'push-ipdatetime')\n",
    "        if ipdatetime_span:\n",
    "            push_time = parse_push_time(ipdatetime_span.text.strip(), post_time.year)\n",
    "\n",
    "        comments.append({\n",
    "            'article_id': article_id,\n",
    "            'push_tag': push_tag,\n",
    "            'user_id': user_id_span.text.strip(),\n",
    "            'content': content_span.text.strip(': ').strip(),\n",
    "            'push_time': push_time\n",
    "        })\n",
    "\n",
    "    return article_info, comments\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_URL = 'https://www.ptt.cc'\n",
    "    BOARD = 'Baseball'\n",
    "    START_DATE = datetime(2020, 1, 1)\n",
    "\n",
    "    if not os.path.exists(DB_NAME):\n",
    "        setup_database()\n",
    "\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.post(f'{BASE_URL}/ask/over18', data={'yes': 'yes'})\n",
    "\n",
    "    last_page_url, latest_crawled_url = get_progress(cursor)\n",
    "\n",
    "    start_url = f'{BASE_URL}/bbs/{BOARD}/index.html'\n",
    "    if last_page_url:\n",
    "        print(f\"從上次進度繼續：{last_page_url}\")\n",
    "        start_url = last_page_url\n",
    "    elif latest_crawled_url:\n",
    "        print(f\"偵測到已有資料，將快速跳過已爬取文章\")\n",
    "\n",
    "    current_url = start_url\n",
    "    stop_scraping = False\n",
    "    articles_processed = 0\n",
    "    articles_skipped = 0\n",
    "    articles_filtered = 0\n",
    "\n",
    "    print(f\"開始爬取 PTT 棒球版，目標日期：{START_DATE.strftime('%Y-%m-%d')} 至今\")\n",
    "\n",
    "    try:\n",
    "        while not stop_scraping and current_url:\n",
    "            print(f\"\\n處理列表頁: {current_url}\")\n",
    "            try:\n",
    "                response = session.get(current_url, timeout=15)\n",
    "                response.raise_for_status()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"無法讀取列表頁，跳過。錯誤: {e}\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            save_progress(cursor, 'last_page_url', current_url)\n",
    "            conn.commit()\n",
    "\n",
    "            articles_on_page = soup.find_all('div', class_='r-ent')\n",
    "            new_articles_on_page = 0\n",
    "\n",
    "            for article_div in tqdm(articles_on_page, desc=\"掃描文章\", leave=False):\n",
    "                title_link = article_div.find('a')\n",
    "                if not title_link:\n",
    "                    continue\n",
    "\n",
    "                article_title = title_link.text\n",
    "                article_url = f\"{BASE_URL}{title_link['href']}\"\n",
    "\n",
    "                # 快速檢查：文章是否已存在（避免重複爬取）\n",
    "                if is_article_exists(cursor, article_url):\n",
    "                    articles_skipped += 1\n",
    "                    continue\n",
    "\n",
    "                # 過濾 LIVE 和公告\n",
    "                should_skip, skip_reason = should_skip_article(article_title)\n",
    "                if should_skip:\n",
    "                    articles_filtered += 1\n",
    "                    tqdm.write(f\"跳過 [{skip_reason}]: {article_title[:30]}...\")\n",
    "                    continue\n",
    "\n",
    "                # 爬取文章\n",
    "                article_info, comments = get_article_data(session, article_url)\n",
    "                time.sleep(0.05)\n",
    "\n",
    "                if article_info is None:\n",
    "                    continue\n",
    "\n",
    "                if article_info['post_time'] < START_DATE:\n",
    "                    stop_scraping = True\n",
    "                    print(f\"\\n文章時間 ({article_info['post_time']}) 早於目標日期，停止爬取\")\n",
    "                    break\n",
    "\n",
    "                # 寫入資料庫\n",
    "                try:\n",
    "                    cursor.execute(\n",
    "                        \"INSERT OR IGNORE INTO articles (article_id, title, author, post_time, board, url) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "                        (article_info['article_id'], article_info['title'], article_info['author'],\n",
    "                         article_info['post_time'], article_info['board'], article_info['url'])\n",
    "                    )\n",
    "                    if comments:\n",
    "                        cursor.executemany(\n",
    "                            \"INSERT INTO comments (article_id, push_tag, user_id, content, push_time) VALUES (?, ?, ?, ?, ?)\",\n",
    "                            [(c['article_id'], c['push_tag'], c['user_id'], c['content'], c['push_time'])\n",
    "                             for c in comments]\n",
    "                        )\n",
    "                    articles_processed += 1\n",
    "                    new_articles_on_page += 1\n",
    "                except sqlite3.Error as e:\n",
    "                    print(f\"資料庫寫入錯誤: {e}\")\n",
    "\n",
    "                # 每 50 篇提交一次\n",
    "                if articles_processed % 50 == 0:\n",
    "                    conn.commit()\n",
    "                    save_progress(cursor, 'last_page_url', current_url)\n",
    "                    conn.commit()\n",
    "                    tqdm.write(f\"已處理 {articles_processed} 篇新文章 (跳過 {articles_skipped} 篇已爬, 過濾 {articles_filtered} 篇)\")\n",
    "\n",
    "            # 如果整頁都是已爬過的文章，加速翻頁\n",
    "            if new_articles_on_page == 0 and articles_skipped > 0:\n",
    "                print(f\"本頁全是已爬文章，快速跳過\")\n",
    "\n",
    "            # 翻到上一頁\n",
    "            prev_page_link = soup.find('a', class_='btn wide', string='‹ 上頁')\n",
    "            current_url = f\"{BASE_URL}{prev_page_link['href']}\" if prev_page_link else None\n",
    "\n",
    "    finally:\n",
    "        if current_url:\n",
    "            save_progress(cursor, 'last_page_url', current_url)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(f\"\\n爬取結束\")\n",
    "        print(f\"  新增文章：{articles_processed} 篇\")\n",
    "        print(f\"  跳過已爬：{articles_skipped} 篇\")\n",
    "        print(f\"  過濾文章：{articles_filtered} 篇 (LIVE/公告)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8dd8490-e8cd-4174-911e-6ca63e899c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  2 12:15:01 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090         Off| 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   36C    P8               14W / 450W|      2MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53556615-9709-4a45-a2a5-4da0e001816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:          125Gi        24Gi       8.9Gi       1.1Gi        92Gi        99Gi\n",
      "Swap:         8.0Gi       173Mi       7.8Gi\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bca2a-a64a-42fb-ae84-288a0a8b3669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
